---
title: 'nhanesA: Achieving Transparency and Reproducibility in NHANES Research'
author: 'Laha Ale, Robert Gentleman, Teresa Filshtein-Sonmez, Christopher Endres'
bibliography: references.bib
output:
  pdf_document: null
  bookdown::pdf_book:
    citation_package: biblatex
  word_document: default
  html_document:
    df_print: paged
header-includes: \usepackage{multicol}
link-citations: yes
csl: vancouver.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Abstract

The National Health and Nutrition Examination Survey (NHANES) covers a broad array of
topics including demography, sociology, health, and nutrition. Since 1999 the
survey has been carried out in two year cycles.  Data collected are publicly
available. This has made NHANES invaluable in areas of research such as social
determinants of health, trends in obesity, and many behaviors and health outcomes.
Assembling the data and analyzing it present a number of technical and analytic challenges.
In this paper we describe the `nhanesA` package which is designed to aid researchers in
obtaining and analyzing the data as well as enabling the sharing and extending of
published analyses.  With our open approach to systematically enhancing the NHANES
data we hope to catalyze community efforts to achieve reproducibility and
accessibility of analytic methods in public health research.

Database URL: <https://github.com/cjendres1/nhanes>

## Introduction

NHANES <https://www.cdc.gov/nchs/nhanes/> is a pivotal program of studies aimed at evaluating the health and nutritional well-being of both adults and children residing in the United States. NHANES combines detailed interviews and thorough physical examinations,
producing hundreds of variable collected over each of twelve cycles from 1999 to 2022. NHANES is administered by the National Center for Health Statistics (NCHS), an integral part of the Centers for Disease Control and Prevention (CDC), which holds the responsibility for generating crucial health and vital statistics for the entire nation.
 
Since 1999, the NHANES survey has been conducted continuously, and the surveys during that period are referred to as "continuous NHANES" to distinguish from several prior surveys. Continuous NHANES surveys are grouped in two-year *cycles*, with the first cycle carried out in 1999-2000.  With some exceptions, all the data are public and are widely used for a variety of different analysis.  Records in PubMed indicate that more than 5,000 papers per year make some use of this resource.  The CDC website provides instructions on downloading and analyzing the data.

The stated objective of NHANES is to produce "national estimates that are
representative of the total noninstitutionalized civilian
U.S. population." [https://www.cdc.gov/nchs/data/series/sr_02/sr02-190.pdf]
The sampling strategy used by NHANES provides researchers with data from citizens that are often missing or under-represented in other epidemiology studies. These include a range of ethnicities, diverse ages ranging from babies to the elderly, oversampling some of the rarer populations. NHANES has been useful as a way to study both prevalence and changes over time in obesity, social determinants of health and other important public health issues.  While each cycle is cross-sectional one can examine the time ordering of cycles to get a sense of how different characteristics of the population change with time. This sampling design introduces complexity into the analysis and researchers must be careful to use appropriate survey methodology.  
% the forward reference to package seems inappropriate here
%To support this the `nhanesA` package includes a vignette describing the appropriate use of these methods and points the reader to a %large variety of on-line resources.
 
We have found that replication of results of papers based on NHANES data remains a demanding endeavor. In our experience even when authors have the best of intentions it is very difficult to replicate the tables and graphs in their papers. This is due to a variety of challenges that largely relate to a lack of specificity in reporting the extent and manner of data cleaning, the details of inclusion criteria or specific phenotypic definitions. Additionally it is difficult to accurately report on the extent to which the data were transformed or filtered during the analysis. While accurate textual description of these processes is hard they can be succinctly and accurately described in software and we propose methods to make it easier to ensure that the software descriptions of the analysis are synchronized with the outputs and easily shared with interested readers.

## Methods

### Data

The publicly available continuous NHANES data consists of over 1000 data tables. Each cycle surveys a distinct set of individuals using a cluster sample approach that is detailed here <https://wwwn.cdc.gov/nchs/nhanes/tutorials/SampleDesign.aspx>. Each NHANES cycle produces data in five categories: Demographics, Dietary, Examination, Laboratory and Questionnaire. There is also limited access data that is not publicly available and requires a formal request for access.

The available data can be downloaded using https requests from the CDC website. For each table there are two components, the raw data which is provided in SAS (Statistical Analysis Software) 'XPT' (Transport File Format) format and a documentation file, in HTML, that describes the data variables and format. 

During the 2019-2020 cycle data collection was disrupted by the pandemic. Therefore, the partial 2019-2020 data (herein 2019-March 2020 data) were combined with the full data set from the previous cycle (2017-2018) to create nationally representative 2017-March 2020 pre-pandemic data files.  These data files have the same basic file name, e.g. `DEMO`, but they are named by prepending a `P_` to that, giving `P_DEMO`. These files require special handling and the CDC has provided substantial guidance as well as updating the survey weights etc. Going forward the CDC plans to revamp the entire survey in the 2023-2024 cycle as documented in @Ram2021.

We next describe a number of the computational and analytic tasks that need to be addressed and show how functionality in `nhanesA` helps address them.

### Search for the variables and data files that you will use to perform the analysis

While the CDC website offers search capabilities, having dedicated R-based functions offers more direct manipulation of research results. In `nhanesA`, we have introduced functions like `nhanesSearch`, `nhanesSearchTableNames`, and `nhanesSearchVarName` to streamline these operations.

### Download the data onto your local computer

Data is download into data frames for subsequent analysis. The function `nhanes` takes the name of the table that is wanted and downloads it. Users can specify whether the data should be translated by setting the argument `translated` to `TRUE`.  In general we recommend that one always translate the data, and the `translated` set to `TRUE` by default.

### Align tables within a cycle or across cycles

After downloading the data, tables tables from different surveys (e.g., demography and dietary) can be synchronized within a cycle using the `merge` function from R using the `SEQN` variable as the match. Additionally, one can align tables across years, such as concatenating the demography survey data from 2009-2010, 2011-2012, and 2013-2014. When aligning across cycles users should be careful to check that they have selected variables that are present in all cycles and where the meanings are the same.  For example, the `RIDAGEYR` variable is present in the cycles mentioned and so could be included when joining across cycles.

### Translate tables

Categorical variables, both ordered and unordered, are encoded in the raw data as integers. In many scenarios, using integers is not intuitive and can lead to rather uninterpretable models. In Figure \ref{fig:DEMO_J} we show the translated and untranslated variables 
`RIAGENDR` and `RIDRETH1` from `DEMO_J`. Now, if one was to use the untranslated variable in a regression model, for example, then the variable would be treated as an integer variable, but it represents a categorical variable, and needs to be encoded appropriately.

\begin{figure*}
  \includegraphics{images/translated.jpg}
  \caption{a) shows the raw data the both gender and ethnicity are encoded as integers. b) shows the translated data with `nhanesA`.}
  \label{fig:DEMO_J}
\end{figure*}


### Use the survey weights to obtain valid estimates

NHANES uses a complex, four-stage sample design and appropriate analyses typically involve using specialized survey analysis procedures that use special weights that account for the sampling scheme that was used to collect the data. Each sample person is assigned a sample weight, reflecting the number of people in the broader population that the individual represents. To obtain valid estimates from the data, it's essential to apply these survey weights during analysis. By doing so, researchers account for the complex survey design and some potential biases, ensuring the results are reflective of the entire population and not just the sampled individuals. There is extensive documentation provided on the CDC website describing the proper use of these weights (https://wwwn.cdc.gov/nchs/nhanes/tutorials/default.aspx). 
We recommend using the 'survey' @JSSv009i08 package to perform these analyses. We provide a simple example later in this paper and more extensive examples in the vignettes for the `nhanesA` and `phonto` @phonto2023 packages.

## An Example

We now demonstrate the use of the `nhanesA` package, together with the `survey` package to look at average blood pressure for individuals over 40 years of age by reported ethnicity for the 2017-2018 cycle.  To do that we need to obtain the demographic data (DEMO_J) and the blood pressure data (BXP_J) and merge them.

```{r loadlibs, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library("nhanesA")
library("survey")
library("knitr")
```

```{r demoj, eval=T}
demoj = nhanes("DEMO_J")
dim(demoj)

## merge DEMO_J and BXP_J using SEQN.  
bpxj = nhanes("BPX_J")
data = merge(demoj, bpxj, by="SEQN")
dim(data)
```

In order to make appropriate estimates of we will need to create a survey design object to incorporate the weights into our analysis. It is essential to create the survey design structure prior to doing any subsetting your data. This ensures that the complex survey design features, such as stratification and clustering, are accurately captured and applied to the entire dataset. The CDC provides detailed explanations on its website at <https://wwwn.cdc.gov/nchs/nhanes/tutorials/weighting.aspx>.


```{r survery}
nhanesDesign <- svydesign(id = ~SDMVPSU,  # Primary Sampling Units (PSU)
                          strata  = ~SDMVSTRA, # Stratification used in the survey
                          weights = ~WTMEC2YR,   # Survey weights
                          nest    = TRUE,      # Whether PSUs are nested within strata
                          data    = data)

```

Next we subset the data to contain only those over 40 years of age. Now we must use the tools in the `survey` package to do any manipulation of the data.  This ensures that appropriate adjustment of weights is made so that subsequent analyses are appropriate.
We also create a second subset, that is not contained within a survey design framework so that it is easy to examine the unadjusted values.


```{r surveydesign}

dfsub = subset(nhanesDesign, data$RIDAGEYR>=40)

datasub = data[data$RIDAGEYR>=40,]

```

Next we will look at the reported ethnicity: variable RIDRETH1 in table DEMO_J.

```{r ethtables}
table(datasub$RIDRETH1) |> kable(col.names=c("Ethnicity", "Freq"))
```

For illustration purposes we examine diasotolic blood pressure and for ease of presentation we only use the first measurement, variable BPXDI1 in tabel BPX_J.
We can compute the unadjusted mean of diastolic blood pressure both for the whole data set and also by ethnicity.
```{r unadjbp}
mean(datasub$BPXDI1, na.rm=TRUE)
##divide the data by
sp1 = split(datasub$BPXDI1, datasub$RIDRETH1)
mns = sapply(sp1, mean, na.rm=TRUE)

mns = data.frame(Ethnicity = names(mns), "Raw DBP"=mns)
kable(mns, row.names=FALSE)
```

Now, we perform the same analysis using the survey weigths. First we get the adjusted over all mean for the population

```{r svyby}
adjmn = svymean(~BPXDI1, dfsub, na.rm=TRUE)
adjmn
# By ethnicity
adjmnsbyEth = svyby(~BPXDI1, ~RIDRETH1, dfsub, svymean, na.rm=TRUE)
kable(adjmnsbyEth[,c(2,3)], col.names=c("Adj DBP","SE"))
##all.equal(names(mns), as.character(adjmnsbyEth$RIDRETH1)

both = cbind(adjmnsbyEth[,c(1,2)], mns[,2] )
kable(both, row.names=FALSE, col.names=c("Ethnicity", "Adj DBP", "Raw DPB"))

```

### Challenging Aspects of the NHANES data

There remain some challenges to analyzing the NHANES data for which there are no easy ways to address the issues. We discuss a number of the issues here in order to alert analysts to their existence so that the analyst can remediate any impacts. 

Within in NHANES there is a substantial amount of missing data. In part this arises from non-response but it can also aruse due to the fact that not all respondents participate in all of the assays, exams or questionnaires.

Some questionnaires are reused across cycles. For example the main demography questionnaire is present in all cycles. The CDC uses some naming conventions, but these are not always consistently applied and it is important to be cautious when merging or combining data across questionnaires or cycles. The names of the variables are not guaranteed to be constant, and the actual questions asked may also change over time. 

The survey questions are delivered sequentially and there is some amount of structure that will need to be dealt with. In the Blood Pressure and Cholestorol questionnaire for 2005-2006 we show an excerpt of the Data Documentation file in Figure. \ref{fig:BPQ_020}. You can see that in question BPQ_020 that anyone who answered either 'No' or 'Don't know' to the question skipped over question BPQ_030, as it makes little sense for them.  Importantly the value stored in the database for those people for that question was a missing value. Now, in some circumstances an analyst might prefer to assume that if the respondent and not been told that they had high blood pressure once, the also had not been told they had high blood pressure two or more times.   There are many instances in the NHANES data where questions are skipped as part of the survey delivery and it is important that the analyst try to detect those and make reasonable assumptions for the analysis. 

\begin{figure*}
  \includegraphics{images/BPQ020D.png}
  \caption{question BPQ\_020.}
  \label{fig:BPQ_020}
\end{figure*}


In the NHANES dataset, data coarsening is frequently observed. For instance, the age variable (RIDAGEYR in DEMO_J) uses a representation where the value 80 denotes individuals aged 80 and above. Similarly, the ratio of family income to poverty (INDFMPIR) uses the value 5 to indicate a ratio greater than or equal to 5.00. These practices compromise the precision of numerical values in the dataset. Additionally, some variables, although expressed numerically, are better interpreted as categorical data. Take the Body Mass Index ($11.5 \sim 67.3 kg/m^2$, represented as BMXBMI in BMX_J) for example. While it is expressed as a continuous number, categorizing it into predefined ranges such as underweight (<18.5), healthy weight (18.5 to <25), overweight (25.0 to <30), and obesity (30.0 or higher) might yield more meaningful analyses.

## Other Approaches

### R Packages

The `NHANES` package @Pruim2015 provides a subset of data from the 2009-2010 and 2011-2012 cycles. The authors have created a small subset of the data for teaching purposes. They have included 75 variables and created two datasets. The `NHANESraw` dataframe is the raw data together with information on the sample weighting scheme. Their `NHANES` dataframe contains 10,000 rows that were resampled from 'NHANESraw' that *undid* the oversampling and hence analyses using `NHANES` can be performed without using the survey weights. The authors are quite explicit that this is a teaching resource and that any scientific investigations should rely on the data from the NHANES CDC site and not on their subset.

The `RNHANES` package @Susmann2016 is produced by the Silent Spring Institute.
RNHANES provides an easy way to download and analyze data from NHANES with a focus on the laboratory data. They provide methods to find all data files and to download them. They provide a search capability as well as making some attempt to obtain the units of measurement for the laboratory data.  The `nhanes_load_data` function provides a method for downloading and merging data, although the features are limited. It also has arguments to allow for recording/translating the factor variables, although that seemed to be very slow to run.  There are good functions that encapsulate the use of the `survey` package, but that seems to be at the expense of flexibility in the analysis.

### Stata

We did not find any Stata modules or packages but there are good resources available on the web, such as those from the Statistical Consulting Unit at UCLA, https://stats.oarc.ucla.edu/stata/seminars/survey-data-analysis-in-stata-17/.

### Python

We are aware of two actively maintained Python libraries for working with NHANES data: `nhanes-dl`(https://pypi.org/project/nhanes-dl ) and `pynhanes` (https://pypi.org/project/pynhanes ). In Python, one can use Jupyter notebooks to achieve reproducible results. Jupyter notebooks, similar to Rmarkdown, allow for an organized presentation of text, code, and their respective outputs (including plots) within a single document. This facilitates reproducibility, enabling readers to easily replicate and understand the presented work. However, the `nhanes-dl` library is designed to download Continuous NHANES codebooks and convert them into ready-to-use pandas dataframes, although its documentation is somewhat lacking. The `pynhanes` offers several Jupyter notebooks on its GitHub repository(https://github.com/timpyrkov/pynhanes/tree/master/scripts) to demonstrate its usage.

## Discussion and future work

NHANES, with its depth and breadth of health and nutritional data, serves as a cornerstone for epidemiologic and health research. However, the intricacies and nuances associated with the data, combined with the varied methodologies employed across different research domains, present considerable analytic challenges.  We have described a number of ways in which `nhanesA` can facilitate analyzing these data and have indicated a number of issues that are not easily addressed in software and remain for the analyst to address. 

We believe that there is additional value to be obtained from the many papers based on NHANES and in particular point out that when the reported analyses are reproducible then they also become extensible in at least two directions. First, when studying population characteristics there is substantial value in being able to repeat an analysis when data from a new cycle are released.  Second, for any analysis, the ability to extend that analysis using additional covariates from other questionnaires, or to explore the impact of unadjusted for covariates (eg. explore social determinants of health) can be very powerful. 
With regard to reproducibility we mean the computational reproducibility of the figures and tables in a paper.  Which essentially means that once the dataset is agreed upon, all analytical outputs can be precisely replicated, while the general scientific reproducibility emphasizes the need to obtain similar results across analogous, though not identical, samples. In the supplement we propose a process that offers a structured approach for researchers using the NHANES dataset. Harnessing the synergy between GitHub, Rmarkdown, and specific packages like `nhanesA`, it sets the stage for a transparent, modular, and rigorously organized research process. Every stage, from data selection to preprocessing decisions and analytical procedures, is systematically recorded and versioned, ensuring transparency and reproducibility.  The essential components of this process have been used to write papers and books by many of the contributors to the Bioconductor Project <www.bioconductor.org> for the past 20 years or so. We believe that it would be valuable to start a community effort to collect and collate papers based primarily on NHANES data that use strategies to encourage reproducibility and extensibility, regardless of the computing language used.

We believe that encapsulating the public NHANES data into a SQL database that is contained in a Docker (cite Docker) container is an important next step.  This would enable faster access, both due to the data being local to the user and also because the use of SQL and various tools that come with databases better support some of the data manipulations. We are working on container that also contains an instance of R and R Studio to further encourage reproducibility of results.  Such an approach will make it easier to add additional data resources and create more complex, and hence valuable data sets.



## Code availability
The `nhanesA` package is available to the public on: https://github.com/cjendres1/nhanes. The current CRAN version is also available at https://github.com/cran/nhanesA.

## Confict of interest.
RG consults broadly in the pharmaceutical and Biotech industries. He owns shares or options in a number of publicly traded and private companies.

## Acknowledgements

We thank Vincent Carey from Harvard Medical School for his review and insights on our paper. Additionally, we thank our colleagues from the Center for Computational Biomedicine: Nathan Palmer, Rafael Goncalves, Jason Payne, and Samantha Pullman, for their efforts in building the Docker database and testing the `nhanesA`.

## Appendix

### Reproducible research

We believe that the `nhanesA` package makes a substantial contribution to enhancing reproducibility and rigor in the scientific process. Here we want to outline a few tools that can be used in conjunction with `nhanesA` to create documents that are reusable and extensible. The reproducibility of a paper, or result, can be enhanced by using a number of tools and processes that are commonly used for software development. We outline thes set of tools that we believe are useful and then give a vignette outlining a simplified paper-writing approach that uses these tools. 

An important development was the concept of *Markdown* @Gruber, which is a straightforward markup language designed for crafting formatted text without the intricacies of HTML. Rmarkdown builds upon Markdown, intertwining it with the R programming language. Essentially, Rmarkdown is an implementation of Markdown, allowing users to embed R code within a document. This fusion supports dynamic reporting, where narrative and code coexist, fostering clear, reproducible research outcomes.

\cite{Xie2018,Allaire2023}  documents which are documents that integrate software (code) and text. These can be thought of as explicit descriptions of how the figures and the tables in the published paper were created. Rmarkdown documents are processed by different *engines* that transform them into specific outputs such as a PDF format for publication or a HTML output for putting on the web. 

A second important tool to help with reproducibility is the use of version control systems. These were originally developed for software development but they work equally well for writing papers. A widely used tool for version control is GitHub (https://github.com/).  One example of using this approach based on R @R-base is the Epidemiologist R Handbook (https://epirhandbook.com/en/) which is written in Bookdown @Xie2020 and is maintained in GitHub (at https://github.com/appliedepi/epiRhandbook_eng). The authors have created an entire textbook using markdown and they use GitHub to handle version issues as well as bug reporting and fixing. This approach has been used widely in the R community for over 20 years with substantial success. It should come as no surprise to the reader that this paper is also written in markdown and uses GitHub as its source code repository (https://github.com/ainilaha/RNhanes).

\begin{figure}
  \includegraphics{images/process.jpg}
  \caption{Workflow for ensuring transparent and reproducible research: (1) Authors use RMarkdown and R files, managed with Git version control for organization and collaboration. The nhanesA package facilitates NHANES access. Git and GitHub facilitate this by archiving and source code control. (2) Work is committed, pushed, and made public on GitHub in the form of Rmarkdown and R files. (3) Any one who wants to reproduce the work can fork or clone the repository to reproduce or expand upon the work. External users can access the NHANES database in the same way as the original authors. Contributions or extensions can be integrated via pull requests and subsequent merging. }
  \label{fig:process}
\end{figure}


#### Markdown

Markdown is a lightweight markup language that uses simple syntax to format text. Designed for readability and simplicity, it enables the creation of well-structured documents without the complexities of HTML. Common uses include README files, forums, and documentation. With Markdown, elements like headers, links, lists, and bold or italic text are easily achieved using non-intrusive syntax.
Markdown Cheat Sheet:https://www.markdownguide.org/cheat-sheet/

Rmarkdown, on the other hand, extends the capabilities of Markdown for the R programming community. It seamlessly integrates the R code with Markdown, allowing users to embed R code chunks within the text. When an Rmarkdown file is executed using tools like knitr, the R code is run, and its outputs (graphs, tables, etc.) are embedded directly into the document. This results in dynamic, interactive reports that combine narrative, code, and output in a single document.
R Markdown Cheat Sheet:https://rmarkdown.rstudio.com/lesson-15.HTML

#### Git and GitHub 

**Git:** At its core, Git is a distributed version control system. It allows multiple users to work on the same project without interfering with each other's changes. By maintaining a history of every modification, Git ensures that users can revert to any previous state of their project, thus facilitating a consistent and error-free development process.

**GitHub:** While Git is the underlying system that tracks changes, GitHub is a web-based platform that hosts Git repositories. It provides a visual interface and additional tools for collaboration, making it easier for researchers and developers to share, discuss, and collaborate on their projects.

Key Concepts:

1. **Repository (Repo)**: A repository is essentially a project's folder containing all files, folders, and relevant data. It also holds the project's revision history. On GitHub, repositories can be public or private, allowing for open-source collaboration or private work, respectively.

2. **Clone**: Cloning refers to creating a copy of a repository from GitHub onto your local machine. This allows researchers to work on their projects offline and synchronize changes later.

3. **Commit**: When you make changes to your project, Git tracks them. Committing is the process of saving these changes to the local repository. Every commit requires a message to briefly describe what was done, ensuring future users (or the future you) understand the project's evolution.

4. **Push**: Once changes are committed locally, they need to be sent or 'pushed' to the GitHub repository. This ensures that your online repository is up-to-date with your local changes.

5. **Merge**: As multiple collaborators work on a project, there will be instances when two or more people modify the same piece of information. Merging is the process of combining different sequences of commits into one unified history, resolving any conflicts that arise.

6. **Branch**: In Git, the main line of development is called the 'main' branch. However, when working on new features or testing out ideas without affecting the main line, users can create a 'branch' or a parallel line of development. Once satisfied with the changes, the branch can be 'merged' back into the main line.

### A simplified NHANES paper writing process

Here we sketch out an outline for writing a paper using the tools we mention in order to create a reproducible paper. By reproducible we really mean that once we have agreed on the data to use, that all of the tables, graphs and other data analyses can be reproduced exactly.  Now this is not the concept of scientific reproducibility where one expects to find a similare result when the basic experiment is repeated on a similar but not identical populations, but it is an important goal in and of itself. 

One would first create a new GitHub repository for the project.
Then, identify the variables of interest and the questionnaire files they are in as well as the cycles (years) of data that will be used.  Create an Rmarkdown document and in that use the `nhanesA` package to download the relevant data.  The author will then check that document into the GitHub repository so that all updates and modifications are noted and so that anyone can check out the document.

At this point you will start to write code chunks in the document to first transform and filter the data according to the entry criteria for your study.  For example, you might want to look at blood pressure on adults over 40. On examining the `BXP` tables you find that two different blood pressure measurements (systolic and diastolic) were recorded at two different time points.  You have to decide how to process those data.  Do you take only one, or do you average both? What about people that have only one measurement? Do you keep them or remove them?  All of these decisions will impact the analysis and the actual values you report in your paper.  By including the code to do this processing in your markdown document and reader can check the code for the actual steps you took.

Then as your research progresses you will manipulate the data to compute different summary statistics, perhaps mean diastolic blood pressure by reported ethnicity.  Again the specific details of how you did that will be maintained in the markdown document.  Ultimately you will have finished your analysis and then arrange the outputs, using the tools available for processing Rmarkdown to produce the final paper for publication.
Then you can submit it. And make sure you commit everything you need (images, tables, text etc) to your GitHub repository.

Once the reviews come back you will update and modify that code and text to reflect the changes that have been asked for. And again you will check in all the files and changes. Once your paper is published you can refer interested parties to your GitHub repository where they can download the markdown documents and rerun them. Perhaps they will make changes to your assumptions to see whether the results change. 

These tools, though demanding an initial learning curve, are intuitive and efficient. As more researchers embrace these practices, the collective reliability and robustness of NHANES-based research will undoubtedly enhance. By fostering an ecosystem of transparent, replicable, and collaborative research, we can reach more informed decisions, richer insights, and a deeper understanding of the NHANES.


## References

